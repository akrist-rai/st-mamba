# Weight Initialization in Deep Learning

Weight initialization determines how neural network weights are set **before training begins**.  
Good initialization prevents **vanishing gradients**, **exploding gradients**, and **slow convergence**.

---

## Why Initialization Matters

If weights are:
- **Too small** â†’ gradients vanish â†’ network stops learning
- **Too large** â†’ gradients explode â†’ unstable training
- **All zeros** â†’ symmetry problem â†’ neurons learn the same thing

ğŸ¯ **Goal:**  
Maintain stable variance of activations and gradients across layers.

---

## Key Terms

- **fan_in**  â†’ number of input connections
- **fan_out** â†’ number of output connections

---

## Xavier (Glorot) Initialization

Designed for **linear, tanh, sigmoid** activations and **attention layers**.

### Xavier Uniform
```python
nn.init.xavier_uniform_(W)

Wâˆ¼U(âˆ’6fanin+fanout,â€…â€Š6fanin+fanout)
Wâˆ¼U(âˆ’faninâ€‹+fanoutâ€‹6â€‹
â€‹,faninâ€‹+fanoutâ€‹6â€‹
â€‹)
Xavier Normal

nn.init.xavier_normal_(W)

Wâˆ¼N(0,â€…â€Š2fanin+fanout)
Wâˆ¼N(0,faninâ€‹+fanoutâ€‹2â€‹
â€‹)
When to Use

    Transformers

    Attention (Query, Key, Value projections)

    Linear layers

    Mamba / SSM models

He (Kaiming) Initialization

Designed for ReLU-based networks.
He Normal (most common)

nn.init.kaiming_normal_(W, nonlinearity="relu")

Wâˆ¼N(0,â€…â€Š2fanin)
Wâˆ¼N(0,faninâ€‹2â€‹
â€‹)
He Uniform

nn.init.kaiming_uniform_(W, nonlinearity="relu")

When to Use

    CNNs

    Deep MLPs

    ReLU / LeakyReLU

âŒ Not recommended for attention layers
LeCun Initialization

Used with SELU activations for self-normalizing networks.

nn.init.normal_(W, mean=0, std=1/\sqrt{fan_{in}})

Orthogonal Initialization

nn.init.orthogonal_(W)

    Preserves vector norms

    Stable gradients

    Common in RNNs and SSMs

Truncated Normal Initialization

Used in many Transformer models (BERT, ViT).

nn.init.trunc_normal_(W, std=0.02)

Prevents rare large weights.
Bias Initialization

nn.init.zeros_(bias)

âœ” Biases are usually initialized to zero
âŒ Weights should never be all zeros
Comparison Table
Initialization	Best Use Case	Activation
Xavier	Attention, Linear	tanh, sigmoid
He	CNNs, Deep Nets	ReLU
LeCun	Self-normalizing nets	SELU
Orthogonal	RNNs, SSMs	Any
Truncated Normal	Transformers	GELU
PyTorch Defaults
Layer	Default Initialization
nn.Linear	Kaiming Uniform
nn.Conv2d	Kaiming Uniform
Attention Layers	Custom (recommended)
Recommended for ST-Mamba / Attention

nn.init.xavier_uniform_(self.W_query.weight)
nn.init.xavier_uniform_(self.W_key.weight)
nn.init.xavier_uniform_(self.W_value.weight)

âœ” Stable attention scores
âœ” Prevents softmax saturation
âœ” Faster convergence
One-Line Summary

    Weight initialization decides whether your network starts learning smoothly, chaotically, or not at all.


---

If you want, next I can:
- Add **mathematical derivations**
- Add **diagrams**
- Customize it for **ST-Mamba specifically**
- Convert this into **notes for exams**

Just say the word ğŸ‘


ChatGPT can make mistakes. Check important info. See Cookie Preferences.
